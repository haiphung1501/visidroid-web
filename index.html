<!DOCTYPE html>
<html>

<head>
    <meta charset="utf-8" />
    <meta name="description" content="Towards Test Generation from Task Description for Mobile
Testing with Multi-modal Reasoning." />
    <meta name="keywords" content="HxAgent" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>Towards Test Generation from Task Description for Mobile
        Testing with Multi-modal Reasoning
    </title>
    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet" />
    <link rel="stylesheet" href="./static/css/bulma.min.css" />
    <link rel="stylesheet" href="./static/css/bulma-carousel.min.css" />
    <link rel="stylesheet" href="./static/css/bulma-slider.min.css" />
    <link rel="stylesheet" href="./static/css/fontawesome.all.min.css" />
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css" />
    <link rel="stylesheet" href="./static/css/index.css" />
    <!-- <link rel="icon" href="./static/images/favicon.svg"> -->
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script defer src="./static/js/fontawesome.all.min.js"></script>
    <script src="./static/js/bulma-carousel.min.js"></script>
    <script src="./static/js/bulma-slider.min.js"></script>
    <script src="./static/js/index.js"></script>
</head>

<body>
    <section class="hero">
        <div class="hero-body">
            <div class="container is-max-desktop">
                <div class="columns is-centered">
                    <div class="column has-text-centered">
                        <h1 class="title is-1 publication-title">Towards Test Generation from Task Description for
                            Mobile Testing with Multi-modal Reasoning

                        </h1>
                        <div class="is-size-5 publication-authors">
                            <span class="author-block">
                                <a href="https://scholar.google.com/citations?user=Erc1mJ8AAAAJ&hl=en">Hieu Huynh</a>
                                <sup>1,2</sup>
                                ,
                            </span>
                            <span class="author-block">
                                <a
                                    href="https://scholar.google.com/citations?view_op=list_works&hl=en&user=646ZOqsAAAAJ">Hai
                                    Phung</a>
                                <sup>1,2</sup>
                                ,
                            </span>
                            <span class="author-block">
                                <a href="https://orcid.org/0009-0003-7633-6731">Hao Pham</a>
                                <sup>1,2</sup>
                                ,

                            </span>
                            <br />
                            <span class="author-block">
                                <a href="https://personal.utdallas.edu/~tien.n.nguyen/">Tien N. Nguyen</a>
                                <sup>3</sup>
                                ,

                            </span>
                            <span class="author-block">
                                <a href="https://nguyenvvu.net/">Vu Nguyen</a>
                                <sup>1,2,4*</sup>
                                .
                            </span>
                        </div>
                        <div class="is-size-5 publication-authors">
                            <span class="author-block">
                                <sup>1</sup>
                                Faculty of Information Technology, University of Science, Ho Chi Minh city, Vietnam
                            </span>
                            <span class="author-block">
                                <sup>2</sup>
                                Vietnam National University, Ho Chi Minh city, Vietnam
                            </span>
                            <span class="author-block">
                                <sup>3</sup>
                                Computer Science Department, University of Texas at Dallas, Dallas, Texas, USA,
                            </span>
                            <span class="author-block">
                                <sup>4</sup>
                                Katalon Inc., Vietnam
                            </span>
                            <span class="author-block">
                                <sup>*</sup> Corresponding author
                            </span>
                            <br />

                        </div>
                        <div class="column has-text-centered">
                            <div class="publication-links">
                                <!-- PDF Link. -->
                                <span class="link-block">
                                    <a href="https://arxiv.org/abs/2504.15917"
                                        class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon">
                                            <i class="fas fa-file-pdf"></i>
                                        </span>
                                        <span>arXiv</span>
                                    </a>
                                </span>
                                <!-- Arxiv PDF link -->
                                <span class="link-block">
                                    <a href="https://www.arxiv.org/pdf/2504.15917" target="_blank"
                                        class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon">
                                            <i class="fas fa-file-pdf"></i>
                                        </span>
                                        <span>Paper</span>
                                    </a>
                                </span>
                                <span class="link-block">
                                    <a href="https://github.com/visidroid/visidroid"
                                        class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon">
                                            <i class="fab fa-github"></i>
                                        </span>
                                        <span>Code</span>
                                    </a>
                                </span>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>


    <section class="section" style="padding-top: 0rem; padding-bottom: 1rem;">
        <div class="container" style="max-width: 1300px;">
            <div class="columns is-centered has-text-centered">
                <div class="column is-four-fifths">
                    <img src="./static/images/overview.png" />
                    <div class="content has-text-centered">
                        <p>This picture presents an overview architecture of VisiDroid. It takes the description of a
                            task as input, automatically interacts with the testing environment
                            to perform the actions on the app, and generates the test scripts
                            without human intervention. VisiDroid maintains experience-based learning by using the
                            Persistent Memory component to store knowledge gained from previously executed similar tasks
                            and the Task Memory component to retain the history and feedback of each step
                        </p>
                    </div>
                    <!-- <img src="./static/images/failed.png" /> -->
                    <!--  -->
                    <!-- <div class="content has-text-justified"> -->
                    <!-- <p> -->
                    <!-- Two examples where the GPT-4 agent failed, along with their -->
                    <!-- screenshot and the accessibility tree of the relevant -->
                    <!-- sections (grey). On the left, the agent fails to proceed to -->
                    <!-- the “Users” section; on the right, the agent repeats -->
                    <!-- entering the same search query. -->
                    <!-- </p> -->
                    <!-- </div> -->
                </div>
            </div>
        </div>
    </section>


    <!-- Paper abstract -->
    <section class="section hero is-light">
        <div class="container is-max-desktop">
            <div class="columns is-centered has-text-centered">
                <div class="column is-four-fifths">
                    <h2 class="title is-3">Abstract</h2>
                    <div class="content has-text-justified">
                        <p> In Android GUI testing, generating an action sequence for a task that can be replayed as a
                            test script is common. Generating se- quences of actions and respective test scripts from
                            task goals de- scribed in natural language can eliminate the need for manually writing test
                            scripts. However, existing approaches based on large language models (LLM) often struggle
                            with identifying the final action, and either end prematurely or continue past the final
                            screen. In this paper, we introduce VisiDroid, a multi-modal, LLM-based, multi-agent
                            framework that iteratively determines the next action and leverages visual images of screens
                            to detect the task’s com- pleteness. The multi-modal approach enhances our model in two
                            significant ways. First, this approach enables it to avoid prematurely terminating a task
                            when textual content alone provides mislead- ing indications of task completion.
                            Additionally, visual input helps the tool avoid errors when changes in the GUI do not
                            directly af- fect functionality toward task completion, such as adjustments to font sizes or
                            colors. Second, the multi-modal approach also ensures the tool not progress beyond the final
                            screen, which might lack explicit textual indicators of task completion but could display a
                            visual element indicating task completion, which is common in GUI apps. Our evaluation shows
                            that VisiDroid achieves an accuracy of 87.3%, outperforming the best baseline relatively by
                            23.5%. We also demonstrate that our multi-modal framework with images and texts enables the
                            LLM to better determine when a task is completed.
                        </p>
                    </div>
                </div>
            </div>
        </div>
    </section>
    <!-- End paper abstract -->

    <section class="section ">
        <div class="container">
            <div class="columns is-centered has-text-centered">
                <div class="column is-four-fifths">
                    <h2 class="title is-3">VisiDroid demo</h2>
                </div>
            </div>
            <div class="video-gallery" style="display: flex; gap: 1rem;">
                <video src="./static/images/change-theme.mp4" autoplay muted loop playsinline controls></video>
                <video src="./static/images/create-contact.mp4" autoplay muted loop playsinline controls></video>
                <video src="./static/images/disable-autosave.mp4" autoplay muted loop playsinline controls></video>
            </div>
        </div>
    </section>

    <section class="section hero is-light">
        <div class="container is-max-desktop">
            <div class="columns is-centered has-text-centered">
                <div class="column is-four-fifths">
                    <h2 class="title is-3">Experiment Results

                    </h2>
                    <img src="./static/images/evaluation.png" />
                    <div class="content has-text-centered">
                        <p>The effectiveness on Generating Sequence of Actions of VisiDroid and baselines on DroidTask.
                        </p>
                    </div>
                    <!-- <img src="./static/images/failed.png" /> -->
                    <!--  -->
                    <!-- <div class="content has-text-justified"> -->
                    <!-- <p> -->
                    <!-- Two examples where the GPT-4 agent failed, along with their -->
                    <!-- screenshot and the accessibility tree of the relevant -->
                    <!-- sections (grey). On the left, the agent fails to proceed to -->
                    <!-- the “Users” section; on the right, the agent repeats -->
                    <!-- entering the same search query. -->
                    <!-- </p> -->
                    <!-- </div> -->
                </div>
            </div>
        </div>
    </section>

    </div>
    </section>
    <section class="section" id="BibTeX">
        <div class="container is-max-desktop content">
            <h2 class="title">BibTeX</h2>
            <pre>
            <code>
@article{huynh2025visidroid,
  title={Towards Test Generation from Task Description for Mobile Testing with Multi-modal Reasoning},
  author={Huynh, Hieu and Phung, Hai and Pham, Hao and Nguyen, Tien N and Nguyen, Vu},
  journal={arXiv preprint arXiv:2504.15917},
  url={https://www.arxiv.org/abs/2504.15917},
  year={2025}
}</code>
        </pre>
        </div>
    </section>
    <footer class="footer">
        <div class="container">
            <div class="columns is-centered">
                <div class="column is-8">
                    <div class="content">
                        <p>
                            This website is licensed under a
                            <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">Creative Commons
                                Attribution-ShareAlike 4.0 International
                                License</a>
                            .

                        </p>
                        <p>
                            This means you are free to borrow the
                            <a href="https://github.com/nerfies/nerfies.github.io">source code</a>
                            of this website, we just ask that you link back to this page in
                            the footer. Please remember to remove the analytics code
                            included in the header of the website which you do not want on
                            your website.

                        </p>
                    </div>
                </div>
            </div>
        </div>
    </footer>
</body>

</html>